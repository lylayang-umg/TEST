{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Segmentation Analysis via RFM+K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose of running this analysis is to analyse the behaviour of customers and make strategies accordingly in order to drive more profits.\n",
    "###  1. What's RFM\n",
    "###  2. Create RFM Table\n",
    "###  3. What's Kmeans clustering\n",
    "###  4. Implement Kmeans clustering\n",
    "###  5. Follow-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 What's RFM (customer value)?\n",
    "### From WIKIPEDIA:\n",
    "RFM is a method used for analyzing customer value. It is commonly used in database marketing and direct marketing and has received particular attention in retail and professional services industries.\n",
    "\n",
    "#### RFM stands for the three dimensions:\n",
    "\n",
    "* Recency – How recently did the customer purchase?-the number of days that have passed since the customer last purchased\n",
    "* Frequency – How often do they purchase?-the number of purchases by the customer in the last 2.5 years \n",
    "* Monetary Value – How much do they spend?- in the last 2.5 years\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFM analysis readily answers these questions for our business…\n",
    "* Who are my best customers?\n",
    "* Which customers are at the verge of churning?\n",
    "* Who has the potential to be converted in more profitable customers?\n",
    "* Who are lost customers that you don’t need to pay much attention to?\n",
    "* Which customers you must retain?\n",
    "* Who are your loyal customers?\n",
    "* Which group of customers is most likely to respond to your current campaign?\n",
    "(Reference from Putler)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate RFM Values:\n",
    "#### We need a few details of each Customers:\n",
    "\n",
    "* Customer ID / Name / Company etc — to identify them\n",
    "* Order date to calculate Recency (R) as days since last purchase: How many days ago was their last purchase? Deduct most recent purchase date from today to calculate the recency value. 1 day ago? 14 days ago? 500 days ago?\n",
    "* Order id to calculate Frequency (F) as total number of transactions: How many times has the customer purchased from our store? For example, if someone placed 10 orders over a period of time, their frequency is 10.\n",
    "* Order value to calculate Monetary (M) as total money spent: How many $$ (or whatever is your currency of calculation) has this customer spent? Simply total up the money from all transactions to get the M value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_profiling as pp\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cluster, tree, decomposition\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import files with pandas\n",
    "path = '/Users/.../data file'\n",
    "all_files = glob.glob(path + '/*.csv', recursive=True)\n",
    "dff = (pd.read_csv(f) for f in all_files)\n",
    "df_up = pd.concat(dff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First 5 rows of dataframe')\n",
    "df_up.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#Row: {}'.format(df_up.shape[0]))\n",
    "print('#columns:{}'.format(df_up.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date formatting\n",
    "df_up['order_date']=pd.to_datetime(df_up['order_date'])\n",
    "df_up['first_order_date']=pd.to_datetime(df_up['first_order_date'])\n",
    "df_up['registered_date']=pd.to_datetime(df_up['registered_date'])\n",
    "df_up['order_year']=df_up['order_date'].dt.year\n",
    "df_up.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3years=df_up.loc[(df_up['order_year']==2016)| (df_up['order_year']==2017)|(df_up['order_year']==2018)]\n",
    "df3years.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select variables for RMF analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new data frame to generate RFM table\n",
    "df3years_sub=df3years[['customer_id','job_order_id','order_date','total_amount','qty']]\n",
    "df3years_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Oldest order date', df3years_sub['order_date'].min())\n",
    "print('Latest order date',df3years_sub['order_date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up end date for RFM analysis\n",
    "import datetime as dt\n",
    "End = dt.datetime(2018, 6, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Generate RFM Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmTable = df3years_sub.groupby('customer_id').agg({'order_date': lambda x: (End - x.max()).days, \n",
    "                                                    'job_order_id': lambda x: len(x), \n",
    "                                                    'total_amount': lambda x: x.sum()}).reset_index()\n",
    "rfmTable['order_date'] = rfmTable['order_date'].astype(int)\n",
    "rfmTable.rename(columns={'order_date': 'recency', \n",
    "                         'job_order_id': 'frequency', \n",
    "                         'total_amount': 'monetary_value'}, inplace=True)\n",
    "print('First 5 rows of RFM Table')\n",
    "rfmTable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = rfmTable[['recency','frequency','monetary_value']].corr()\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the correlation table, we can tell there is high correlation between frequency and monetary_value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Check Correlation between RFM features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 What's KMeans clustering?\n",
    "### K-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups). The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable K.-DataScience.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 How does KMeans work?\n",
    "### The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarity.\n",
    "#### Steps:\n",
    "* Step 1: Initialisation – K initial centroids (cluster center) are generated randomly\n",
    "* Step 2: Assignment – Assign observations to the nearest cluster center via finding the min(distance between the points and the clusters. (arg min D(Pi,Kn), different definition of distance).\n",
    "* Step 3: Move the centroids to their new position– For each cluster that we got in Step2, we recompute its centorid position by averaging all features of all data points within that cluster. (New Centroid Position= sum(vectors(Pi))/sum(i) for each cluster).\n",
    "* Step 4: Repeat Step 3 & Step 4, iteratively until convergence (centroids don't more any more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Use KMeans Clustering to run customer segamentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "data=rfmTable.copy()\n",
    "var_list=['recency','frequency','monetary_value']\n",
    "for i in var_list:\n",
    "    data[i]=preprocessing.scale(data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many clusters? - determine K\n",
    "A good clustering is one that achieves:\n",
    "High within-cluster similarity\n",
    "Low inter-cluster similarity\n",
    "\n",
    "We will use elbow method to find the optimal K. The idea of the elbow method is to run k-means clustering on the dataset for a range of values of k (say, k from 1 to 10 in the example below), and for each value of k calculate the sum of squared errors (SSE).The idea is that we want a small SSE, but that the SSE tends to decrease toward 0 as we increase k (the SSE is 0 when k is equal to the number of data points in the dataset, because then each data point is its own cluster, and there is no error between it and the center of its cluster). So our goal is to choose a small value of k that still has a low SSE, and the elbow usually represents where we start to have diminishing returns by increasing k.\n",
    "\n",
    "WCSSE= SUM(distance(Pi, K1)^2) + SUM(distance(Pi, K2)^2) + SUM(distance(Pi, K3)^2)+... (each Pi within that K cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "#Finding the optimum number of clusters for k-means classification\n",
    "WCSSE = []\n",
    "K = range(1,11)\n",
    "for k in K:\n",
    "    kmeanModel = cluster.KMeans(n_clusters=k,random_state=123)\n",
    "    kmeanModel.fit(data[var_list])\n",
    "    WCSSE.append(kmeanModel.inertia_) #value of the inertia criterion:sum of squared distances to the closest centroid for all observations in the training set\n",
    "\n",
    "cluster_df=pd.DataFrame({'num_clusters':K, 'SSE within Cluster':WCSSE})\n",
    "print(cluster_df)\n",
    "# Plot the elbow\n",
    "plt.plot(K, WCSSE, marker='b*-')\n",
    "plt.xlabel('Numbers of clusters (k)')\n",
    "plt.ylabel('Sum of Squared Error (Within Cluster)')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "'But what happened if the sample sizes are very different across all clusters? We need to normalize our data when calculate WCSSE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the optimum number of clusters for k-means classification\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cluster, tree, decomposition\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "distortions = []\n",
    "K = range(1,11)\n",
    "for k in K:\n",
    "    kmeanModel = cluster.KMeans(n_clusters=k,).fit(data[var_list])\n",
    "    kmeanModel.fit(data[var_list])\n",
    "    distortions.append(sum(np.min(cdist(data[var_list], kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / data[var_list].shape[0])\n",
    "     #normalization term / data.shape[0]\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, marker='b*-')\n",
    "plt.xlabel('Numbers of clusters (k)')\n",
    "plt.ylabel('Distortions')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Number of clusters\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "# Fitting the input data\n",
    "kmeans = kmeans.fit(X)\n",
    "# Getting the cluster labels\n",
    "labels = kmeans.predict(X)\n",
    "# Centroid values\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Comparing with scikit-learn centroids\n",
    "print(C) # From Scratch\n",
    "print(centroids) # From sci-kit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the plot above, we got our optimal k=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k=6\n",
    "km6 = cluster.KMeans(n_clusters=6,random_state=123)\n",
    "data2=data.copy()\n",
    "data2['cluster'] = km6.fit_predict(data2[var_list])\n",
    "print('First 10 rows')\n",
    "data2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km6.fit(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many customers are assigned into each cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_k6 = pd.merge(rfmTable, data2[['customer_id','cluster']], on='customer_id')\n",
    "df_k6.groupby('cluster')['customer_id'].count().reset_index(name='# of customers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans finds outliers nicely and groups them into one cluster~ will check by visulization next~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use boxplot to visualize each cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"cluster\", y=\"frequency\", data=df_k6).set_title('Order Counts of each cluster')  # RUN PLOT   \n",
    "plt.figure(1, figsize=(9, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"cluster\", y=\"monetary_value\", data=df_k6).set_title('Total Sales of each cluster') # RUN PLOT   \n",
    "plt.figure(1, figsize=(9, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at mean value within each clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_k6[var_list+['cluster']].groupby('cluster').mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at median value within each clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_k6[var_list+['cluster']].groupby('cluster').median().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put them together\n",
    "grouped=df_k6.groupby(['cluster']).agg({ 'cluster' : ['count'],\n",
    "        'recency': ['mean', 'median', 'min'] ,\n",
    "        'frequency': ['mean', 'median', 'min'],\n",
    "       'monetary_value': ['mean', 'median', 'min']}).reset_index()\n",
    "grouped.columns = [\"_\".join(x) for x in grouped.columns.ravel()] \n",
    "grouped.sort_values('cluster_count', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster =4 & 5 are selected due to its high stat of Frequency (Transcation) and Monetary_value (Total Sales) and generate a high value customer list for Facebook LookAlike campaign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Follow-up\n",
    "* Use other clustering method to find optimial K when elbow method doesn't work\n",
    "* Test different customer groups (cluster=0, 1 and 3) via retargeting campaign for repeat customers.\n",
    "* Retention team needs to review KPIs and has a working session with analytics team in order to deploy marketing strategy and plan. \n",
    "### 6. Other analysis for retention team\n",
    "* Shopping basket analysis based on per order details. Figure out which products that customer would like to purchase together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check statistic within all groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.ProfileReport(df_k6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
